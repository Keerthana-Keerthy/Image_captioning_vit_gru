# Image_captioning_vit_gru

## Project Overview
This project focuses on building an advanced Image Captioning Model using state-of-the-art machine learning techniques. The core components of the model include a Vision Transformer (ViT) for extracting high-quality visual features from images and a Gated Recurrent Unit (GRU) for generating meaningful and descriptive captions.

## Key Steps
1. Data Preprocessing
2. Model Architecture
   Vision Transformer (ViT): Utilized as the encoder to extract rich, high-dimensional features from images.
   GRU Decoder: Deployed as the decoder to process visual features and generate corresponding text captions.
3. Pipeline Creation
4. Model Training

## Conclusion
After extensive training and evaluation, the image captioning model achieved a final loss value of 0.87. While the model was able to generate captions for the images, the results were not entirely accurate or meaningful in all cases. The generated captions showed some alignment with the image content but lacked coherence and grammatical correctness in certain instances.

<img width="381" alt="img_cap_pred1" src="https://github.com/user-attachments/assets/0fce4655-1196-4bdd-83bb-5ee599dab099" />

<img width="483" alt="img_cap_pred2" src="https://github.com/user-attachments/assets/826fb855-4865-4f3b-9d39-8f66e6861ed8" />

